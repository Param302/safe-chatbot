prompts:
  - task: self_check_input
    content: |
      Your task is to check if the user message below contains any toxic or harmful language, or if it pertains to any of the restricted topics listed below. Respond with 'Yes' if it does, otherwise respond with 'No'.

      <BEGIN RESTRICTED TOPICS>
      - Politics
      - Illegal activities
      - Violence
      - Hate speech
      - Discrimination
      - Adult content
      - Weapons
      - Drugs
      - Sensitive personal information
      <END RESTRICTED TOPICS>

      
      <BEGIN TOXICITY INDICATORS>
      - Profanity
      - Hate speech
      - Threats
      - Harassment
      - Insults
      - Offensive language
      - Discriminatory remarks
      <END TOXICITY INDICATORS>

      User message: "{{ user_input }}"

      Question: Should the user message be blocked (Yes or No)?
      Answer:
  
  - task: self_check_output
    output_parser: yes_no_parser
    content: |
      Your task is to evaluate the assistant's response on two criteria:

      1. Length Check:
        Determine if the response exceeds the maximum allowed length of 300 words.

      2. Citation Check:
        Determine if the response contains any factual claims (dates, numbers, names, events) and if so, verify whether proper citations or references are included.

      Response: "{{ system_output }}"

      Question: Is the response compliant with both criteria (Yes or No)?
      Answer:

